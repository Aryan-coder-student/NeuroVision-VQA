# Visual Question Answering (VQA) System

<div align="center">
  <img src="https://img.shields.io/badge/Framework-BLIP-blue" alt="BLIP Framework" />
  <img src="https://img.shields.io/badge/Built%20with-PyTorch-orange" alt="PyTorch" />
  <img src="https://img.shields.io/badge/Pipeline-DVC-green" alt="DVC Pipeline" />
  <img src="https://img.shields.io/badge/Deployment-Docker-lightblue" alt="Docker" />
  <img src="https://img.shields.io/badge/Agent-LangChain-purple" alt="LangChain" />
</div>
<div style="display:justify-content;">
  <img src="https://github.com/user-attachments/assets/2d6bf915-cecf-4482-bde1-859eaf5fa399">
</div>
https://github.com/user-attachments/assets/bddc6585-1ae1-43fd-aa12-065e2a3f7031


## ğŸ–¥ï¸ Frontend Implementation (âš ï¸ Under Maintenance)

The project's frontend is maintained in a separate repository by [OoONANCY](https://github.com/OoONANCY) and [ANSHIKA1220](https://github.com/ANSHIKA1220) for modular development and better maintenance. The frontend provides an intuitive user interface for interacting with both the VQA model and medical chatbot functionality.

### Frontend Repository

```
https://github.com/OoONANCY/Omkar_frontend
```

The frontend implementation offers:

- User-friendly interface for uploading images
- Interactive question input for VQA functionality
- Chat interface for medical queries
- Responsive design for desktop and mobile devices
- Real-time display of model predictions and responses

To integrate the frontend with the API:

1. Clone the frontend repository
```bash
git clone https://github.com/OoONANCY/Omkar_frontend.git
cd Omkar_frontend
```

2. Install dependencies and run the frontend application
```bash
npm install
npm start
```

3. Ensure the Flask API is running at the expected endpoint (default: http://localhost:5000)

Hereâ€™s your updated **Table of Contents** with a dataset download script added:  

---

## ğŸ“‘ Table of Contents
- [Dataset Download](#-dataset-download)
- [Project Structure](#-project-structure)
- [Core Components](#-core-components)
- [API Architecture](#-api-architecture)
- [LangChain Agent System](#-langchain-agent-system)
- [Docker Setup](#-docker-setup)
- [API Documentation](#-api-documentation)
- [DVC Pipeline](#-dvc-pipeline)
- [Installation](#-installation)
- [Dataset Download](#-dataset-download)
- [Results & Future Improvements](#-results--future-improvements)

---

## ğŸ“¥ Dataset Download  
The VQA-RAD dataset is available on Hugging Face: [flaviagiammarino/vqa-rad](https://huggingface.co/datasets/flaviagiammarino/vqa-rad).  
To download the **VQA-RAD** dataset from Hugging Face and store it in `data/bronze/`, run the following script:  

```python
from datasets import load_dataset

dataset = load_dataset("flaviagiammarino/vqa-rad")
dataset.save_to_disk("data/bronze/")
```

This ensures that your dataset is properly downloaded and stored in the `bronze` data layer. ğŸš€

This VQA model enables machines to understand and answer natural language questions about images. Leveraging the BLIP framework, it achieves high accuracy in interpreting visual content and generating relevant textual responses. The system is enhanced with a LangChain-powered agent for handling medical queries beyond image analysis.

## ğŸ“ Project Structure

```
VQA/
â”‚â”€â”€ .dvc/                 # DVC-related files for data versioning
â”‚â”€â”€ config/               # Configuration files
â”‚â”€â”€ dags/                 # DAG workflows for pipeline execution
â”‚â”€â”€ data/                 # Dataset storage
â”‚   â”œâ”€â”€ bronze/           # Raw data
â”‚   â”œâ”€â”€ silver/           # Processed data
â”‚   â”œâ”€â”€ .gitignore        # Ignore large files
â”‚   â”œâ”€â”€ bronze.dvc        # DVC tracking file for raw data
â”‚â”€â”€ Deployment/           # Deployment-related files
â”‚   â”œâ”€â”€ __pycache__/      # Python cache files
â”‚   â”œâ”€â”€ Model_14...       # Saved fine-tuned model
â”‚   â”œâ”€â”€ streamlit/        # Streamlit UI
â”‚   â”‚   â”œâ”€â”€ .gitignore    # Ignore unnecessary files in Streamlit
â”‚   â”‚   â”œâ”€â”€ .env          # Environment variables
â”‚   â”‚   â”œâ”€â”€ main.py       # Streamlit app entry point
â”‚   â”œâ”€â”€ app.py            # Flask API entry point
â”‚   â”œâ”€â”€ test_api.py       # API testing script
â”‚â”€â”€ logs/                 # Logs for debugging and tracking
â”‚â”€â”€ mlruns/               # MLflow experiment tracking
â”‚â”€â”€ models/               # Directory for storing trained models
â”‚â”€â”€ notebooks/            # Jupyter notebooks for model exploration
â”‚â”€â”€ plugins/              # Additional utilities or extensions
â”‚â”€â”€ results/              # Evaluation results storage
â”‚â”€â”€ src/                  # Core source code
â”‚   â”œâ”€â”€ __pycache__/      # Python cache files
â”‚   â”œâ”€â”€ test/             # Test scripts
â”‚   â”œâ”€â”€ model.py          # Model loading and inference
â”‚   â”œâ”€â”€ preprocess_data.py # Data preprocessing script
â”‚   â”œâ”€â”€ train.py          # Model training script
â”‚   â”œâ”€â”€ evaluate.py       # Model evaluation script
â”‚â”€â”€ .dvcignore            # Ignore files for DVC
â”‚â”€â”€ .gitignore            # Ignore unnecessary files for Git
â”‚â”€â”€ config.yaml           # Main configuration file
â”‚â”€â”€ docker-compose.yml    # Docker Compose setup
â”‚â”€â”€ Dockerfile            # Docker container setup
â”‚â”€â”€ dvc.yaml              # DVC pipeline configuration
â”‚â”€â”€ dvc.lock              # DVC lock file
â”‚â”€â”€ params.yaml           # Hyperparameter settings
â”‚â”€â”€ README.md             # Project documentation
â”‚â”€â”€ requirements.txt      # Python dependencies
```

## ğŸ§© Core Components

### `model.py`

- âš™ï¸ Loads the **BLIP model for Visual Question Answering**
- ğŸ”„ Initializes the model and processor
- âš¡ Fetches model configurations from `config.yaml`
- ğŸ“¤ Returns both the model and its processor for inference and training

### `train.py`

- ğŸ‹ï¸ Handles **model training** using **PyTorch**
- ğŸ“Š Loads preprocessed datasets
- ğŸš€ Implements **gradient accumulation** and **mixed precision training**
- ğŸ“ˆ Tracks **training loss, validation loss, and BLEU score**
- ğŸ“ Uses **MLflow** for experiment tracking
- ğŸ›‘ Implements **early stopping** to save the best model
- ğŸ’¾ Saves the trained model in `Deployment/Model/`

### `evaluate.py`

- âš–ï¸ Loads the trained model and the test dataset
- ğŸ¯ Generates answers using the model
- ğŸ“ Computes **BLEU scores** to measure model performance
- ğŸ“ Saves evaluation results as **JSON files** in `results/`
- ğŸ” Compares the last saved model with the best-performing model

### `preprocess_data.py`

- ğŸ“¥ Loads the **VQA dataset**
- ğŸ–¼ï¸ Converts images to RGB format and tokenizes text data
- ğŸ”„ Uses the BLIP processor to encode images and questions
- ğŸ“¤ Saves the processed data as **pickle files** in `data/silver/`

## ğŸŒ API Architecture

### `app.py` - Flask API Implementation

The Flask API serves as the backbone of our deployment strategy, offering two primary endpoints:

1. **Image Question Answering (`/predict/`)**
   - Receives an image file and a question
   - Processes the image using the BLIP processor
   - Passes the processed inputs to the fine-tuned model
   - Returns the generated answer as a JSON response

2. **Medical Chatbot (`/chat/`)**
   - Accepts a text query related to medical topics
   - Forwards the query to a LangChain-powered agent
   - Returns comprehensive medical information from multiple sources

**Key Components:**
- **CORS Support**: Enables cross-origin requests for frontend integration
- **Environment Variables**: Securely loads API keys for external services
- **GPU/CPU Detection**: Automatically selects the appropriate device for model inference
- **Error Handling**: Implements robust exception handling for all endpoints

## ğŸ¤– LangChain Agent System

Our system employs a sophisticated LangChain agent architecture to handle medical queries beyond image analysis:

### Agent Components

1. **Search Tools**
   - **Medical Web Search**: Utilizes SerpAPI to search the web for medical information related to brain, CT, and MRI scans
   - **PubMed Search**: Connects to PubMed API for accessing peer-reviewed medical research papers

2. **LLM Backend**
   - Powered by Groq's **Gemma2-9b-it** model for generating coherent and accurate responses
   - Configured for medical domain specialization

3. **Memory System**
   - Implements `ConversationBufferMemory` to maintain context across multiple queries
   - Enables follow-up questions and contextual understanding

4. **Agent Configuration**
   - Uses `ZERO_SHOT_REACT_DESCRIPTION` agent type for reasoning capabilities
   - Limited to 10 iterations to ensure timely responses
   - Includes error handling for parsing issues

5. **Flow Process**
   - Receives user query via the `/chat/` endpoint
   - Agent analyzes the query and determines which tools to use
   - Searches appropriate sources (web or PubMed)
   - Synthesizes information into a comprehensive response
   - Returns formatted answer to the user

## ğŸ³ Docker Setup

### `Dockerfile`

The `Dockerfile` defines the containerized environment for deploying the VQA model. It installs all dependencies, copies source files, and sets up both the **Flask API** and **Streamlit UI**.

#### **Dockerfile Breakdown:**

```dockerfile
FROM python:3.10
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY . .
EXPOSE 5000 8501
CMD ["sh", "-c", "python Deployment/app.py & while ! nc -z localhost 5000; do sleep 1; done; streamlit run Deployment/streamlit/main.py --server.port=8501 --server.address=0.0.0.0"]
```

#### **Build and Run Commands:**

```bash
# Build the Docker image
docker build -t vqa-app .

# Run the container
docker run -p 5000:5000 -p 8501:8501 vqa-app
```

## ğŸ“¡ API Documentation

### Predict Answer Endpoint

```bash
# Request format
curl -X POST "http://127.0.0.1:5000/predict/" \
  -F "file=@path/to/image.jpg" \
  -F "question=What is in the image?"
```

**Response:**
```json
{
  "answer": "The image shows a brain MRI scan with a visible tumor in the temporal lobe"
}
```

### Chatbot Query Endpoint

```bash
# Request format
curl -X POST "http://127.0.0.1:5000/chat/" \
  -H "Content-Type: application/json" \
  -d '{"query": "What is Astrocytoma?"}'
```

**Response:**
```json
{
  "response": "Astrocytoma is a type of brain tumor that develops from star-shaped cells called astrocytes. These cells are part of the glial tissue, which supports and protects neurons in the brain and spinal cord. Astrocytomas can range from slow-growing (low-grade) to aggressive (high-grade) tumors. They are classified by the World Health Organization (WHO) into four grades (I-IV), with grade IV being the most aggressive form, also known as glioblastoma multiforme. Symptoms may include headaches, seizures, memory problems, and changes in behavior, depending on the tumor's location and size. Treatment typically involves surgery, radiation therapy, and chemotherapy, depending on the grade and location of the tumor."
}
```

### API Testing (`test_api.py`)

```python
import requests

# Test the chat endpoint
url = "http://127.0.0.1:5000/chat/"
payload = {"query": "What is Astrocytoma?"}
response = requests.post(url, json=payload)
print(response.json())
```

## ğŸ“¦ DVC Pipeline

### **Data Version Control Pipeline**

DVC manages data pipelines efficiently, tracking data, models, and experiments while ensuring reproducibility.

#### **Pipeline Stages:**

1. **Data Preprocessing Stage** ğŸ”„
   - **Input:** Raw data from `data/bronze/`
   - **Process:** `preprocess_data.py` cleans and prepares the dataset
   - **Output:** Processed data stored in `data/silver/`

2. **Model Training Stage** ğŸ‹ï¸
   - **Input:** Preprocessed data from `data/silver/` and hyperparameters from `params.yaml`
   - **Process:** `train.py` trains the VQA model using PyTorch
   - **Output:** Trained model saved in `Deployment/Model/`

3. **Model Evaluation Stage** âš–ï¸
   - **Input:** Trained model from `Deployment/Model/` and test dataset from `data/silver/`
   - **Process:** `evaluate.py` evaluates model performance using BLEU scores
   - **Output:** Evaluation results saved in `results/`

### **DVC Pipeline Configuration (`dvc.yaml`)**

```yaml
stages:
  preprocess:
    cmd: python src/preprocess_data.py
    deps:
      - src/preprocess_data.py
      - data/bronze/
    outs:
      - data/silver/
  
  train:
    cmd: python src/train.py
    deps:
      - src/train.py
      - data/silver/
      - params.yaml
    outs:
      - Deployment/Model/
  
  evaluate:
    cmd: python src/evaluate.py
    deps:
      - src/evaluate.py
      - Deployment/Model/
      - data/silver/
    outs:
      - results/
```

### **DVC Commands**

```bash
# Execute all pipeline stages in sequence
dvc repro

# Push data and models to remote storage
dvc push

# Pull data from remote storage
dvc pull
```

## ğŸ“Œ Installation

Install all required dependencies with:

```bash
pip install -r requirements.txt
```

## ğŸ“Š Results & Future Improvements

### Current Results

- ğŸ“ˆ Logs comprehensive metrics via MLflow
- ğŸ“ Stores detailed evaluation results in `./results/`
- ğŸ’¾ Saves the fine-tuned model in `Deployment/Model/`

### âœ¨ Future Work

- ğŸ”¬ Experiment with different model architectures
- ğŸ“Š Add visualization tools for better model interpretation
- ğŸŒ Expand language support for multilingual VQA
- ğŸ”„ Implement continuous learning capabilities
- ğŸ§  Enhance the LangChain agent with more specialized medical tools
- ğŸ” Add image segmentation capabilities for more detailed medical image analysis

---

<div align="center">
  <p><b>Visual Question Answering Model</b> | Powered by BLIP Framework</p>
</div>
<div align="center">
  <img src="https://img.shields.io/badge/Framework-BLIP-blue" alt="BLIP Framework" />
  <img src="https://img.shields.io/badge/Built%20with-PyTorch-orange" alt="PyTorch" />
  <img src="https://img.shields.io/badge/Agent-LangChain-purple" alt="LangChain" />
</div>
